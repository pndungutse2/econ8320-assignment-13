{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Customization vs Rapid Development\n",
    "\n",
    "As we know from (painful?) experience, Python is powerful because of its ability to leverage `numpy` and `scipy` to implement any statistical model from scratch. We can write the requisite matrix algebra, or the relevant likelihood function, and from there can optimize our model, calculate confidence intervals, and report the output of that model through data frames, lists, or printed tables. Building our own models is great! We get to build a model based on the exact context and assumptions of our problem, and therefore get exactly the model that we wanted. Unfortunately, it takes a LOT of time!\n",
    "\n",
    "This lesson will provide our first exposure to pre-written statistical modeling in Python. We will be able to use only a couple of lines of code to implement complex and valuable statistical and machine learning models. Because the most costly asset in programming is the time that we spend debugging and writing code (running code is MUCH faster and cheaper than the time spent writing code), we are always looking for ways to avoid writing code that someone else has already written.\n",
    "\n",
    "`statsmodels` is a library that covers the majority of regression models commonly used by economists and statisticians in other fields.\n",
    "\n",
    "`sklearn` is an analogous library that covers machine learning models (aside from deep neural networks, which have their own implementations).\n",
    "\n",
    "Each of these libraries is highly optimized to provide performant implementations of models that we use regularly, and allow us to avoid writing these models from scratch unless we need to customize our model for some specific use case! This is great news! You'll never have to think about writing your own linear or logistic regression from scratch again!\n",
    "\n",
    "Let's dive in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` makes statistics in Python easy! The library contains tools for regressions ranging from linear regression, to logistic regression, count regressions (negative binomial and poisson), various options for robust covariance measures, and tools to implement time series models as well! There are also really useful tools for assisting in creating our regression model based on any structure that best suits us.\n",
    "\n",
    "We can import `statsmodels` in one of two ways:\n",
    "\n",
    "1) With support for R-style formulas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
    "      import pandas.util.testing as tm\n",
    "\n",
    "\n",
    "This is probably the best way to import our data if we are doing regression analysis for causal inference. In these cases, we are not typically trying to make predictions as new data arrives, and so we do not need to have tools ready to analyze new data using our existing regression models.\n",
    "\n",
    "2) Import `statsmodels` to use pre-built numpy arrays as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have other tools that we can use, but we need to manually arrange our `x` and `y` matrices. It looks clunky at first, but can be useful when we are building predictive pipelines using regression models, or when we might want to use both `statsmodels` and `sklearn` with the same data source.\n",
    "\n",
    "Let's start with option 1..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using formulas, we prepare our dataset by importing the data into a Pandas `DataFrame`. We should take care that each of our variables has a name with \n",
    "1) **No spaces**\n",
    "2) No symbols\n",
    "3) Made up of letters and numbers (also can't have a number as the first character)\n",
    "\n",
    "Our code so far might look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that our data set has already been cleaned. If our data has not yet been cleaned, then we need to clean our data prior to working with either `statsmodels` or `sklearn`. This is because regression AND machine learning models require that all information be provided in numeric format. We need to transform text-based data into categorical data (using either ordered numeric columns or binary variable columns generated from our categories), and ensure that all data is represented in the way that we want to use it within our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Equations\n",
    "\n",
    "`statsmodels` incorporates `R`-style regression equations by using the `patsy` library behind the scenes. We will talk more about `patsy` soon. The pattern for regression equations is as follows:\n",
    "\n",
    "```\"dependent variable ~ independent variable + another independent variable + any other independent variables\"```\n",
    "\n",
    "The regression equation will be stored in a string (unlike in `R`), and we put our dependent variable (also called the endogenous variable, or outcome of interest) in the leftmost position within the string. We separate the dependent variable from all independent (exogenous or explanatory) variables using the `~` symbol. Then, each independent variable is separated from the others using `+` operators. \n",
    "\n",
    "The reason is is so important that our column names be properly cleaned before implementing regression analysis is that spaces and other problematic formats for column names will cause problems with our regression equations.\n",
    "\n",
    "### Implementing a Model\n",
    "\n",
    "The first model we might try is a simple linear regression. These are the most common regression models, and typically what someone is referring to when they discuss \"running a regression\". The code is wonderfully simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols(\"hhincome ~ year\", data=data).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:               hhincome   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:24:28   Log-Likelihood:            -1.7131e+05\n",
    "    No. Observations:               13712   AIC:                         3.426e+05\n",
    "    Df Residuals:                   13711   BIC:                         3.426e+05\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept      0.0188      0.000    138.414      0.000       0.019       0.019\n",
    "    year          37.8564      0.274    138.414      0.000      37.320      38.392\n",
    "    ==============================================================================\n",
    "    Omnibus:                     9819.620   Durbin-Watson:                   1.027\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):           250725.793\n",
    "    Skew:                           3.151   Prob(JB):                         0.00\n",
    "    Kurtosis:                      22.978   Cond. No.                     9.31e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 6.41e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "When we run these two lines of code, we are creating, fitting, and reporting on a regression model! It's fast, it's clean, and it's really easy to implement! `sm.ols` is the OLS class of regression models, and takes two required arguments: a regression equation (passed as a string), and a data source (expected to be a `pandas.DataFrame` object). We use the `.fit()` method to complete all of the math that actually solves our regression model. When we call `.summary()` on a fitted regression, we get a printout of the regression summary tables for the model, complete with diagnostic measures, estimates of our beta coefficients, and confidence intervals!\n",
    "\n",
    "If the model is satisfactory, then we are done! (It really is that simple!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to keep iterating on my model, I might want to try regressing year on the logged average household incomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year\", data=data[data['hhincome']>0]).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:31:18   Log-Likelihood:                -17363.\n",
    "    No. Observations:               13653   AIC:                         3.473e+04\n",
    "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
    "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
    "    ==============================================================================\n",
    "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
    "    Skew:                          -1.469   Prob(JB):                         0.00\n",
    "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "As you can see from the code above, everything is the same, except that we were able to transform household income using `np.log` on the go! We don't even need to create a new column! We can just do it inside of our regression model! We also subset our data so that the log operator doesn't break our model by introducing $-\\infty$ as a possible `hhincome` value.\n",
    "\n",
    "In other cases, it might be useful to create state-level fixed effects by including dummy variables for the states in our `statefip` column. Note that this won't work with our current data, since we only have one state in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data[data['hhincome']>0]).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:       np.log(hhincome)   R-squared:                      -0.000\n",
    "    Model:                            OLS   Adj. R-squared:                 -0.000\n",
    "    Method:                 Least Squares   F-statistic:                      -inf\n",
    "    Date:                Wed, 16 Mar 2022   Prob (F-statistic):                nan\n",
    "    Time:                        15:32:19   Log-Likelihood:                -17363.\n",
    "    No. Observations:               13653   AIC:                         3.473e+04\n",
    "    Df Residuals:                   13652   BIC:                         3.474e+04\n",
    "    Df Model:                           0                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept   2.698e-06   1.82e-09   1481.190      0.000    2.69e-06     2.7e-06\n",
    "    year           0.0054   3.67e-06   1481.190      0.000       0.005       0.005\n",
    "    ==============================================================================\n",
    "    Omnibus:                     5172.537   Durbin-Watson:                   1.277\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):            63678.616\n",
    "    Skew:                          -1.469   Prob(JB):                         0.00\n",
    "    Kurtosis:                      13.164   Cond. No.                     8.12e+17\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "    [2] The smallest eigenvalue is 8.4e-26. This might indicate that there are\n",
    "    strong multicollinearity problems or that the design matrix is singular.\n",
    "\n",
    "\n",
    "    /opt/conda/lib/python3.7/site-packages/statsmodels/regression/linear_model.py:1657: RuntimeWarning: divide by zero encountered in double_scalars\n",
    "      return self.ess/self.df_model\n",
    "\n",
    "\n",
    "The `C()` command indicates that we would like to consider the `statefip` variable as a **C**ategorical variable, not a numeric variable. We can transform ANY column using the categorical operator. It is most useful when a column is text-based, or when a column is numeric but should not be treated as a count, ordinal, or continuous variable. We CAN use it on our dependent variable, but this will (unless our dependent variable was binary text data) break our regression model, which expects only a single dependent variable, rather than an array of dependent variables.\n",
    "\n",
    "Sometimes we want to include transformed variables in our model without creating a new column. The `I()` operator allows us to do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square a variable using the I() function for\n",
    "#   mathematical transformations\n",
    "reg = smf.ols(\"np.log(hhincome) ~ age + I(age**2)\", data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we transform `age` by squaring it (maybe in preparation to create an age-earnings profile?). One line, simple syntax, what could be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine variables using the I() function for\n",
    "#   mathematical transformations\n",
    "reg = smf.ols(\"np.log(hhincome) ~ I(age-education-5)\", data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example combines TWO columns to create a new measure (proxying experience by subtracting education from age, and subtracting an additional 5 years). All we have to do is describe the relationship that we want to model as an explanatory variable, and we are off to the races! Most operators are fair game, and we can include an arbitrary number of columns in our measure calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More robust modeling\n",
    "\n",
    "If we want to utilize robust standard errors, we can easily update our regression results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
    "# Use White's (1980) Standard Error\n",
    "reg.get_robustcov_results(cov_type='HC0')\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if we want to cluster our standard errors by state,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = smf.ols(\"np.log(hhincome) ~ year + C(statefip)\", data=data).fit()\n",
    "# Use Cluster-robust Standard Errors\n",
    "reg.get_robustcov_results(cov_type='cluster', groups=data['statefip']) # Need to specify groups\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to stick to just `HC0` and cluster-robust standard errors. Below are some of the [covariance options](http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.get_robustcov_results.html) that we have:\n",
    "1) `HC0`: White's (1980) Heteroskedasticity robust standard errors\n",
    "2) `HC1`, `HC2`, `HC3`: MacKinnon and White's (1985) alternative robust standard errors, with `HC3` being designed for improved performance in small samples\n",
    "3) `cluster`: Cluster robust standard errors\n",
    "4) `hac-panel`: Panel robust standard errors\n",
    "\n",
    "We should choose the standard errors that best fit our specific data needs, and it is important to realize that this choice is highly context-dependent. The structure and nature of our data should be carefully considered, as should the specific regression model that we are trying to implement.\n",
    "\n",
    "### Time Series Models\n",
    "\n",
    "Not only can we model linear regression, we also have multiple time series options available. We won't go into much detail, since each of these models deserve to have significant time devoted to them, and we just don't have the time in this class.\n",
    "\n",
    "- [ARIMA](http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html) models\n",
    "- [VAR](http://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.var_model.VAR.html) models\n",
    "- [Exponential Smoothing](https://www.statsmodels.org/stable/tsa.html#exponential-smoothing) models\n",
    "\n",
    "We can run an ARIMA, for example, using code like the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't work unless we have multiple years of data (which we currently don't)\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "y = data.loc[data['statefip']==31, ['hhincome','year']]\n",
    "y.index=pd.to_datetime(y.year)\n",
    "reg = ARIMA(y['hhincome'], order=(1,1,0)).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Discrete Outcomes\n",
    "\n",
    "If we have a [binary dependent variable](https://www.statsmodels.org/devel/discretemod.html), we are able to use either [Logit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit) or [Probit](https://www.statsmodels.org/devel/generated/statsmodels.discrete.discrete_model.Probit.html#statsmodels.discrete.discrete_model.Probit) models to estimate the effect of exogenous variables on our outcome of interest. To fit a Logit model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "myformula=\"married ~ hhincome + C(statefip) + C(year) + educ\"\n",
    "model= sm.Logit.from_formula(myformula, data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Count Data\n",
    "\n",
    "When modeling count data, we have options such as [Poisson](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Poisson.html#statsmodels.discrete.discrete_model.Poisson) and [Negative Binomial](http://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.NegativeBinomial.html#statsmodels.discrete.discrete_model.NegativeBinomial) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8310/raw/master/DataSets/auto-mpg.csv\")\n",
    "\n",
    "myformula=\"nchild ~ hhincome + C(statefip) + C(year) + educ + married\"\n",
    "\n",
    "model= sm.Poisson.from_formula(myformula, data=data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other regression \"flavors\", and the best way to learn about what is available through `statsmodels` is to [read the docs](https://www.statsmodels.org/stable/user-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `patsy` library\n",
    "\n",
    "We have been using regression equations in `statsmodels` a lot without really discussing what is happening behind the scenes. `statsmodels` relies on a library called `patsy` to parse regression equations and prepare our data for regression analysis. While `statsmodels` does a great job of incorporating the `patsy` library for us, this isn't always the case. In fact, it is a really valuable tool in many other contexts (think machine learning or deep learning). \n",
    "\n",
    "\n",
    "### Why use `patsy`?\n",
    "\n",
    "We don't necessarily have to use `patsy`. We could just select our variables manually. Creating a column of ones to serve as our intercept column is trivial (you of course remember that from the linear regression assignment). `patsy` is a tool for creating a standardized pipeline to deal with data that is stored in identical formats, and aids us in creating reusable or replicable code. Patsy allows us to separate our endogenous and exogenous variables AND to\n",
    "\t- \"Dummy out\" categorical variables\n",
    "\t- Easily transform variables (square, or log transforms, etc.)\n",
    "\t- Use identical transformations on future data\n",
    "    \n",
    "Even better, `patsy` is just as easy to use as regression equations. We just need to learn about the function wrappers that are necessary to create our processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy as pt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/Econ8320/blob/master/AssignmentData/assignment8Data.csv?raw=true\")\n",
    "\n",
    "# To create y AND x matrices\n",
    "y, x = pt.dmatrices(\"hhincome ~ year + educ + married + age\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get started, we need to import `patsy`, and we typically give it the two-letter abbreviation `pt`. Once we have imported our data, we use the `pt.dmatrices` function. This function takes a regression equation (again, as a string), and a data source. The returned value is a **tuple** of `y` and `x`. We can break that tuple into two values by using the `y, x = ...` syntax, so that we have a `y` array and an `x` array.\n",
    "\n",
    "We don't have to create BOTH `y` and `x` data, though! We can use the `pt.dmatrix` function to just create an `x` matrix. Maybe we already have a dependent variable, and want to try out variations on our explanatory variables to see how each performs. In this case, our regression equation should have no column name to the left of the `~` symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create ONLY an x matrix\n",
    "x = pt.dmatrix(\"~ year + educ + married + age\", data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more note is that these regression equations automatically include an intercept term. If you do NOT want an intercept term (some regression models and most machine learning models don't use them), then you can add `-1` as an exogenous variable in your regression equation, in order to indicate that you want to eliminate the column of ones that make up the intercept column in our matrix of exogenous regressors.\n",
    "\n",
    "### Categorical Variables\n",
    "\n",
    "Again, we have the functions described in the regression section above available to us as we transform our data. We can create categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create y AND x matrices\n",
    "eqn = \"hhincome ~ C(year) + educ + married + age\"\n",
    "y, x = pt.dmatrices(eqn, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And (again) we can transform variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create y AND x matrices\n",
    "eqn = \"I(np.log(hhincome)) ~ C(year) + educ + married + age + I(age**2)\"\n",
    "y, x = pt.dmatrices(eqn, data = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use interaction operators. `*` will interact each value of two columns, and also include the original columns in the regression model. `:` will include only the interaction terms, while omitting the original columns. Check out the [explanation of formulas](https://patsy.readthedocs.io/en/latest/formulas.html) for more details.\n",
    "\n",
    "\n",
    "### SUPER IMPORTANT $\\rightarrow$ Same Transformation on New Data!\n",
    "\n",
    "Often, we will want to build a model with observed data that can make predictions about new observations as those observations are recorded. `patsy` provides a simple function to take the structure of one exogenous matrix and generate another identically structured matrix using new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a new x matrix based on our previous version\n",
    "xNew = pt.build_design_matrices([x.design_info], dataNew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we can create a new matrix in the SAME SHAPE as our original `x` matrix by using the `build_design_matrices()` function in `patsy`. \n",
    "\n",
    "We pass a list containing the old design matrix information (because we can actually create many matrices simultaneously), as well as the new data from which to construct our new matrix.\n",
    "\n",
    "Why does recreating our `x` array matter? This process ensures that we always have the same number of categories in our categorical variables. A new, smaller subset of data that is freshly observed may not contain observations of every category, in which case an updated patsy matrix would not contain the correct number of columns! We are able to maintain consistency in our model, making our work replicable. Most importantly, this will streamline the use of `statsmodels` and `sklearn` in the same workflow!\n",
    "\n",
    "Speaking of `sklearn`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn`\n",
    "\n",
    "What `statsmodels` does for regression analysis, `sklearn` does for predictive analytics and machine learning. It is a truly fabulous library. `sklearn` is likely the most popular machine learning library, and has a standard API to make using the library VERY simple. Even better, it's documentation is some of the nicest documentation you will find anywhere, and contains incredible detail about how to implement models, as well as lessons about the \"how\" and \"why\" of using each model. You couldn't write a better textbook about machine learning than the documentation for `sklearn`.\n",
    "\n",
    "Below, we will briefly discuss some of the models that are most commonly utilized from `sklearn`. Details will be sparse. We are mostly focused on the code implementation of these models. More detail on how machine learning models work is provided in  Business Forecasting, and is outside the scope of this course.\n",
    "\n",
    "### Decision Tree Classification (and Regression)\n",
    "\n",
    "[Classification](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) and [Regression](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) Trees (CARTs) are the standard jumping-off point for exploring machine learning. They are very easy to implement in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import patsy as pt\n",
    "\n",
    "data = pd.read_csv(\"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/roomOccupancy.csv\")\n",
    "\n",
    "y, x = pt.dmatrices(\"Occupancy ~ CO2\", data=data)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In-sample accuracy: 0.9753162225224119\n",
    "\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "We also implement [Support Vector Machines](http://scikit-learn.org/stable/modules/svm.html#svm) for both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    /opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
    "      \"avoid this warning.\", FutureWarning)\n",
    "\n",
    "\n",
    "    In-sample accuracy: 0.9397028122313643\n",
    "\n",
    "\n",
    "Can you see the API pattern yet?\n",
    "\n",
    "### Random Forest Models\n",
    "\n",
    "Again, available in both [classification](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [regression](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) flavors, these models are aggregations of many randomized Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf = clf.fit(x, y.squeeze())\n",
    "\n",
    "pred = clf.predict(x)\n",
    "\n",
    "print(\"In-sample accuracy: {}\".format(accuracy_score(y.squeeze(), pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In-sample accuracy: 0.9748250030701215\n",
    "\n",
    "\n",
    "There MUST be a pattern here...\n",
    "\n",
    "Of course there is! We import our classifier (or regressor), then create an instance of that object. We can name it `clf` or anything else that we prefer. From there, the process is the same:\n",
    "- Use the `.fit()` method, passing in the relevant data for our context\n",
    "- Create predictions using our fitted model with `.predict()` and new exogenous data (or the old data to test in-sample fit)\n",
    "- Measure the performance of our model with `accuracy_score`, or any other metric that can describe performance given a specific use case\n",
    "\n",
    "### More from `sklearn`\n",
    "\n",
    "Many other tools are also available to aid in the data cleaning process through `sklearn`. Some of these are:\n",
    "\n",
    "- [Principal Component Analysis (PCA)](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n",
    "- [Factor Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis)\n",
    "- Many [Cross-Validation Algorithms](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Hyperparameter Tuning](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "   - Finding the correct parameters for a decision tree or random forest, for example\n",
    "- [Model Evaluation Tools](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Plotting decision trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Using the wage data provided here (https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv), create a linear regression model to explain and/or predict wages. Your data set should be labeled `data` and your fitted model should be stored as `reg`. If you do not name the model correctly, you won't get any points!\n",
    "\n",
    "Please put all your code for this exercise in the cell labeled `#si-linear-regression` file found below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T15:17:58.549170Z",
     "start_time": "2024-11-23T15:17:58.273093Z"
    }
   },
   "source": [
    "#si-linear-regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data_url = \"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/wagePanelData.csv\"\n",
    "data = pd.read_csv(data_url)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(data.head())\n",
    "\n",
    "# Preprocess the data: Handle missing values and select relevant columns\n",
    "# Assuming 'wage' is the target variable and other columns are predictors\n",
    "data = data.dropna()  # Remove rows with missing values\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=['log_wage'])  # Drop the 'wage' column for predictors\n",
    "y = data['log_wage']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Display the coefficients of the model\n",
    "print(\"Coefficients:\", reg.coef_)\n",
    "print(\"Intercept:\", reg.intercept_)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  year  years_experience  weeks_worked  occupation_code  industry_code  \\\n",
      "0   1     1                 3            32                0              0   \n",
      "1   1     2                 4            43                0              0   \n",
      "2   1     3                 5            40                0              0   \n",
      "3   1     4                 6            39                0              0   \n",
      "4   1     5                 7            42                0              1   \n",
      "\n",
      "   south_region  metropolitan_resident  ms  female  union_member  education  \\\n",
      "0             1                      0   1       0             0          9   \n",
      "1             1                      0   1       0             0          9   \n",
      "2             1                      0   1       0             0          9   \n",
      "3             1                      0   1       0             0          9   \n",
      "4             1                      0   1       0             0          9   \n",
      "\n",
      "   is_black  log_wage  \n",
      "0         0   5.56068  \n",
      "1         0   5.72031  \n",
      "2         0   5.99645  \n",
      "3         0   5.99645  \n",
      "4         0   6.06146  \n",
      "Mean Squared Error: 0.1022753089108999\n",
      "Coefficients: [-1.65757027e-04  9.06679793e-02  6.20258071e-03  3.19436940e-03\n",
      " -1.39485659e-01  4.89381996e-02 -6.89004807e-02  1.73266483e-01\n",
      "  1.04347798e-01 -3.50983451e-01  9.32896341e-02  5.13612563e-02\n",
      " -1.44013639e-01]\n",
      "Intercept: 5.321592882292487\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Import the pass/fail data for students in Portugal found here(https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv), and create a logistic regression model \n",
    "using `statsmodels` that can estimate the likelihood of students passing or failing class. The dependent variable is contained in the column called `G3`, which takes the value `1` when the student has a passing final grade, and `0` otherwise.\n",
    "\n",
    "Call your fitted model `reg`, and place all code for this exercise in the cell labeled `#si-logistic-regression` file found below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T15:20:33.533548Z",
     "start_time": "2024-11-23T15:20:29.219798Z"
    }
   },
   "source": [
    "#si-logistic-regression\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the dataset\n",
    "data_url = \"https://github.com/dustywhite7/pythonMikkeli/raw/master/exampleData/passFailTrain.csv\"\n",
    "data = pd.read_csv(data_url)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(data.head())\n",
    "\n",
    "# Preprocess the data: Handle missing values and ensure correct data types\n",
    "data = data.dropna()  # Remove rows with missing values\n",
    "\n",
    "# Define the dependent variable (G3) and independent variables\n",
    "X = data.drop(columns=['G3'])  # Features\n",
    "X = sm.add_constant(X)  # Add a constant term for the intercept in the model\n",
    "y = data['G3']  # Target variable\n",
    "\n",
    "# Fit the logistic regression model\n",
    "reg = sm.Logit(y, X).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(reg.summary())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  school  sex  age  address  famsize  Pstatus  Medu  Fedu  Mjob  \\\n",
      "0          16       0    0   16        1        0        1     4     4     3   \n",
      "1          66       0    1   15        1        0        0     4     4     2   \n",
      "2         211       0    1   17        1        1        1     4     4     3   \n",
      "3           7       0    0   17        1        0        0     4     4     2   \n",
      "4          19       0    1   16        1        1        1     4     3     1   \n",
      "\n",
      "   ...  famrel  freetime  goout  Dalc  Walc  health  absences  G1  G2  G3  \n",
      "0  ...       3         2      3     1     2       2         6   1   1   1  \n",
      "1  ...       1         3      3     5     5       3         4   1   1   1  \n",
      "2  ...       5         3      5     4     5       3        13   1   1   1  \n",
      "3  ...       4         1      4     1     1       1         6   0   1   0  \n",
      "4  ...       3         1      3     1     3       5         4   0   1   1  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.313771\n",
      "         Iterations 25\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                     G3   No. Observations:                  296\n",
      "Model:                          Logit   Df Residuals:                      262\n",
      "Method:                           MLE   Df Model:                           33\n",
      "Date:                Sat, 23 Nov 2024   Pseudo R-squ.:                  0.4980\n",
      "Time:                        09:20:33   Log-Likelihood:                -92.876\n",
      "converged:                       True   LL-Null:                       -185.01\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.314e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -37.8386        nan        nan        nan         nan         nan\n",
      "Unnamed: 0     0.0002      0.003      0.058      0.953      -0.007       0.007\n",
      "school        -0.2226      0.799     -0.279      0.781      -1.789       1.344\n",
      "sex            0.4512      0.532      0.849      0.396      -0.591       1.493\n",
      "age           -0.6683      0.286     -2.340      0.019      -1.228      -0.109\n",
      "address       -0.2726      0.534     -0.511      0.610      -1.319       0.774\n",
      "famsize       -0.0655      0.457     -0.143      0.886      -0.961       0.830\n",
      "Pstatus       -0.6200      0.683     -0.908      0.364      -1.959       0.719\n",
      "Medu           0.1185      0.292      0.406      0.685      -0.453       0.690\n",
      "Fedu          -0.1614      0.255     -0.632      0.528      -0.662       0.339\n",
      "Mjob          -0.2220      0.190     -1.166      0.243      -0.595       0.151\n",
      "Fjob           0.0388      0.238      0.163      0.871      -0.428       0.506\n",
      "reason         0.1785      0.178      1.001      0.317      -0.171       0.528\n",
      "guardian       0.3266      0.385      0.848      0.396      -0.428       1.081\n",
      "traveltime     0.2017      0.307      0.658      0.511      -0.399       0.803\n",
      "studytime     -0.1899      0.309     -0.615      0.538      -0.795       0.415\n",
      "failures      -0.4699      0.282     -1.669      0.095      -1.022       0.082\n",
      "schoolsup     -0.4917      0.569     -0.864      0.387      -1.607       0.623\n",
      "famsup        -1.0076      0.493     -2.042      0.041      -1.975      -0.041\n",
      "paid           0.7327      0.460      1.594      0.111      -0.168       1.634\n",
      "activities    -0.3223      0.436     -0.740      0.460      -1.177       0.532\n",
      "nursery       -0.4218      0.544     -0.775      0.438      -1.489       0.645\n",
      "higher         1.4193      1.004      1.413      0.158      -0.549       3.388\n",
      "internet      -0.0501      0.550     -0.091      0.927      -1.128       1.028\n",
      "romantic      -0.5576      0.441     -1.264      0.206      -1.423       0.307\n",
      "famrel         0.3461      0.247      1.404      0.160      -0.137       0.829\n",
      "freetime      -0.0304      0.208     -0.146      0.884      -0.438       0.377\n",
      "goout         -0.3697      0.208     -1.776      0.076      -0.778       0.038\n",
      "Dalc          -0.2911      0.277     -1.051      0.293      -0.834       0.252\n",
      "Walc           0.1672      0.245      0.683      0.495      -0.313       0.647\n",
      "health        -0.0620      0.153     -0.404      0.686      -0.363       0.238\n",
      "absences      -0.0433      0.022     -1.986      0.047      -0.086      -0.001\n",
      "G1             3.6013      0.489      7.370      0.000       2.644       4.559\n",
      "G2            49.1039        nan        nan        nan         nan         nan\n",
      "==============================================================================\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve-it!\n",
    "\n",
    "Use the data on NFL franchise values included in the NFL Valuation data source (https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv) file to implement a Random Forest Classifier in sklearn using 100 trees to predict team-years when `Playoffs` takes the value `1` (when a team made the playoffs in that season).\n",
    "\n",
    "- Use Patsy to create `x2` and `y2` matrices\n",
    "- Create the classifier\n",
    "- Fit the classifier, and store the fitted model with the name `playoffForest`\n",
    "\n",
    "Place all code for this exercise in the cell labeled `#si-random-forest` file found below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T15:54:11.315480Z",
     "start_time": "2024-11-23T15:54:11.124150Z"
    }
   },
   "source": [
    "#si-random-forest\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data_url = \"https://raw.githubusercontent.com/dustywhite7/Econ8320/master/AssignmentData/assignment12Data.csv\"\n",
    "data = pd.read_csv(data_url)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(data.head())\n",
    "\n",
    "# Preprocess the data: Handle missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Use Patsy to create feature (x2) and target (y2) matrices\n",
    "formula = 'Playoffs ~ Value + Revenues + Expansion + OperatingIncome + Expansion + LaborContract'\n",
    "y2, x2 = dmatrices(formula, data, return_type='dataframe')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the Random Forest Classifier with 100 trees\n",
    "playoffForest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "playoffForest.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = playoffForest.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance on test and prediction\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Team  Teamno  Year  Value  ChangeValue  DebtToValue  \\\n",
      "0        Dallas Cowboys       1  1999    663         61.0            6   \n",
      "1   Washington Redskins       2  1999    607         51.0           82   \n",
      "2  Tampa Bay Buccaneers       3  1999    502         45.0           28   \n",
      "3     Carolina Panthers       4  1999    488         34.0           34   \n",
      "4  New England Patriots       5  1999    460         83.0           37   \n",
      "\n",
      "   Revenues  OperatingIncome  Expansion  TVDeal  LaborContract  Playoffs  \\\n",
      "0     161.7             56.7          0       1              0         1   \n",
      "1     151.8             48.8          0       1              0         0   \n",
      "2     128.7             41.2          0       1              0         0   \n",
      "3     128.5             18.8          0       1              0         0   \n",
      "4     107.6             13.5          0       1              0         1   \n",
      "\n",
      "   SuperBowl  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.74      0.67        68\n",
      "         1.0       0.33      0.23      0.27        40\n",
      "\n",
      "    accuracy                           0.55       108\n",
      "   macro avg       0.48      0.48      0.47       108\n",
      "weighted avg       0.51      0.55      0.52       108\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
